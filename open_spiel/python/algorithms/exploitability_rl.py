# Copyright 2019 DeepMind Technologies Limited
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Tests for open_spiel.python.algorithms.dqn."""

from absl.testing import absltest
import tensorflow.compat.v1 as tf

from open_spiel.python import rl_environment
from open_spiel.python.algorithms import dqn
import pyspiel
import os

# Temporarily disable TF2 behavior until code is updated.
tf.disable_v2_behavior()

class exploitability_rl(object):

  def __init__(self, base_agents, env):
    self.base_agents = base_agents
    self.env = env
    # dqn超参数
    self.base_value_loop_nums = 25000
    self.BR_value_loop_nums = 25000
    self.dqn_loop_nums = 25000
    # self.dqn_save_every = 100
    self.hidden_layers_sizes = [256, 256]
    self.replay_buffer_capacity = int(2e5)
    self.num_train_episodes = int(1e5)
    self.num_players = len(self.base_agents)
    self.rl_agents = {}
    self.base_values = {}
    self.BR_values = {}



  # 使用nfsp求解出的源策略进行多次对局，取每个玩家的收益平均值作为拟合出的博弈树根节点value值
  def get_base_values(self):
    sum = {}
    for i in range(self.num_players):
      sum[i] = 0 
    for ep in range(self.base_value_loop_nums):
      time_step = self.env.reset()
      while not time_step.last():
        player_id = time_step.observations["current_player"]
        info_state = time_step.observations["info_state"][player_id]
        legal_actions = time_step.observations["legal_actions"][player_id]
        
        # print("info_state",info_state)
        action, probs = self.base_agents[player_id]._act(info_state, legal_actions)
        time_step = self.env.step([action])
      for i in range(self.num_players):
        sum[i] += time_step.rewards[i]
      
    for i in range(self.num_players):   
      self.base_values[i] = sum[i] / self.base_value_loop_nums


  # 依次替换掉导入模型中的每个玩家（用dqn智能体替代），训练得到每个玩家对应源策略的BR策略
  def train_and_get_BR_values(self):
    sum = {}
    for i in range(self.num_players):
      sum[i] = 0 
      
    for BR_player in range(self.num_players):
      with tf.Session() as sess:
        state_representation_size = self.env.observation_spec()["info_state"][0]
        num_actions = self.env.action_spec()["num_actions"]
        hidden_layers_sizes = [int(l) for l in self.hidden_layers_sizes]
        kwargs = {
          "replay_buffer_capacity": self.replay_buffer_capacity,
          "epsilon_decay_duration": self.num_train_episodes,
          "epsilon_start": 0.06,
          "epsilon_end": 0.001,
        }
        # base_dir = "/repo/open_spiel/python/examples/exploitability_rl/"
        # # 保存文件名
        # save_dir = os.path.join(base_dir, f"BR_model_saved_player_{BR_player}")
        # if not os.path.exists(save_dir):
        #   os.makedirs(save_dir)

        rl_agent = dqn.DQN(sess, BR_player, state_representation_size,
                              num_actions, hidden_layers_sizes, **kwargs)
        sess.run(tf.global_variables_initializer())

        for _ in range(self.dqn_loop_nums):
          time_step = self.env.reset()
          while not time_step.last():
            player_id = time_step.observations["current_player"]
            if player_id == BR_player:
              agent_output = rl_agent.step(time_step)
              time_step = self.env.step([agent_output.action])
            else:
              info_state = time_step.observations["info_state"][player_id]
              legal_actions = time_step.observations["legal_actions"][player_id]
              action, probs = self.base_agents[player_id]._act(info_state, legal_actions)
              time_step = self.env.step([action])  

          rl_agent.step(time_step)

        # 循环结束，保存模型  
        # rl_agent.save(save_dir)
        self.rl_agents[BR_player] = rl_agent
        
        for ep in range(self.BR_value_loop_nums):
          time_step = self.env.reset()
          while not time_step.last():
            player_id = time_step.observations["current_player"]
            if player_id == BR_player:
              agent_output = self.rl_agents[player_id].step(time_step, is_evaluation=True)
              time_step = self.env.step([agent_output.action])
            else:
              info_state = time_step.observations["info_state"][player_id]
              legal_actions = time_step.observations["legal_actions"][player_id]
              action, probs = self.base_agents[player_id]._act(info_state, legal_actions)
              time_step = self.env.step([action])

          sum[BR_player] += time_step.rewards[BR_player]
          
    for i in range(self.num_players):   
      self.BR_values[i] = sum[i] / self.BR_value_loop_nums      


  def exploitability_mp_rl(self):
    self.get_base_values()
    self.train_and_get_BR_values()

    BestResponsePolicy_Opponent = self.BR_values[0]
    base_value_Opponent = self.base_values[0]
    BestResponsePolicy_Team = sum(self.BR_values[best_responder]
            for best_responder in range(1, self.num_players))
    base_value_Team = sum(self.base_values[best_responder]
            for best_responder in range(1, self.num_players))
    
    nash_conv_value_opponent = BestResponsePolicy_Opponent - base_value_Opponent
    nash_conv_value_team = (BestResponsePolicy_Team - base_value_Team)/(self.num_players - 1)
    nash_conv_value = nash_conv_value_opponent + nash_conv_value_team

    print("BestResponsePolicy_Opponent: ",BestResponsePolicy_Opponent)
    print("base_value_Opponent: ",base_value_Opponent)
    print("BestResponsePolicy_Team: ",BestResponsePolicy_Team)
    print("base_value_Team: ",base_value_Team)
    
    return nash_conv_value / 2
  
  
